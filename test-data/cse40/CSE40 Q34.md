# Q34 Cheat Sheet

#### Terms:

- **Bayesian** probability: specifies the degree of uncertainty that the user has about an event.  It is sometimes referred to as “subjective probability” or “degree of belief.”
- **Frequentist** probability: considers the relative frequencies of events of interest to the total number of events that occurred.  The probability of an event is defined as the relative frequency of the event in the limit when one has infinite data.
- **Outcomes**: mutually exclusive and disjoint (exactly one outcome occurs)
- **Sample Space**:  the set of all possible (disjoint) outcomes of an experiment.
- **Event**: A subset of outcomes (potential results of an experiment)
  - Events can overlap (any given outcome can be part of mutliple events)
- **Probability**: likelihood of an outcome in a sample space occurring on a single trial
- **Random Variable**: a mapping from outcomes in the sample space to values
  - Random variables have **domains** (sets of possible values)
- **Discrete**: finite set of possible outcomes
- **Continuous**: infinite set of possible outcomes
- **Probability Distributions**: a distribution over a set of a *single* random variable
- **Joint Probability Distributions**: a distribution over a set of *multiple* random variables
- **Marginal Distributions**: A sub distribution of a joint distribution to a single variable.
  - Marginalization: Combining collapsed rows by adding.
- **Conditional Distributions**: probability distributions over some variables, given fixed values of others.
- U**niform Distribution**: all outcomes are equally probable: $P(x) = {1 \\over |X|}$
- Two events are **independent** if knowing whether one event occurred doesn't change the probability of the other event.
- **Inference**: given a joint distribution, we can reason about unobserved variables given observations (evidence). !\[\[Screenshot 2025-02-09 at 2.50.41 PM.png|400\]\]
- **Conditional Independence**:
  - Correlated
  - $B\\rightarrow A$ and $C$ : Confounder
  - If $A$ changes, $C$ is not likely to change !\[\[Screenshot 2025-02-09 at 2.55.20 PM.png|400\]\]
- **Conditional Dependence**:
  - $A$ and $C$ are independent
  - If $A$ changes, $C$ is likely to change (if we know the battery is fine the probability of being out of gas given the car does not start increases) !\[\[Screenshot 2025-02-09 at 2.56.12 PM.png|400\]\]
- **Correlation**: A relationship or connection between two things based on co-occurrence or pattern of change - a tendency for two variables to change together
  - If variables are correlated they are **NOT** independent
- **Statistical Correlation**: any statistical relationship, whether causal or not, between two random variables
  - Usually refers to the degree to which a pair of variables are *linearly* related
- **Causality**: Influence by which one event, process, state, or object (a cause) contributes to the production of another event, process, state, or object (an effect) where the cause is partly responsible for the effect, and the effect is partly dependent on the cause.
- Random Controlled Trial **RCT**: a group of subjects is randomly partitioned into the control group and a treatment group. (usually*double-blind* trials)
- **Confounding Factors**: Factors that can cause difficulty identifying causality
- **Correlation** helps with prediction; if X and Y are positively correlated, then observing high X means we should see high Y.
- **Causation**: needed for decision making; if X and Y are causally connected, then if I manipulate the value of X, keeping everything else constant, the value of Y will change.
- **Confounder**: correlation is often due to confounding latent variable that is a hidden cause of both X and Y
- **Simpson's Paradox**: a phenomenon in probability and statistics, in which a trend appears in several different groups of data but disappears or reverses when these groups are combined.
  - two variables may be positively associated in a population, but be independent or even negatively associated in all subpopulations.
  - Examples include epidemiology and studies of discrimination, where understanding the paradox is essential for drawing the correct conclusions from the data.
- **Selection Bias**: there are systematic differences between subsets of data, such that the subset is not representative of the overall population.
  - **Sampling Bias**: the non-random sampling of a population, e.g., the sample does not accurately represent the characteristics of the population from which it was drawn.
  - **Exclusion Bias**: the researcher intentionally removes some subgroups from the sample population.
  - **Attrition Bias**: some research participants exit the study while it’s still ongoing. If this happens differentially for different subpopulations, this affects the quality of the outcomes arrived at in the end.
  - **Survivorship Bias**: the researcher screens subjects and chooses the ones who pass the screening process successfully.

!\[\[Screenshot 2025-02-09 at 3.14.57 PM.png|400\]\]

#### Equations:

- Sum Rule: Given $P(X,Y) \\rightarrow P(x) = \\sum\\limits\_{y\\in Y} P(x,y)$
- Product Rule: $P(x|y) = {P(x,y)\\over P(y)} \\leftrightarrow P(x,y) = P(x|y)P(y)$
- Probability of a Disjunction (A or B): $P(A\\cup B)=P(A)+P(B)-P(A\\cap B)$
- Bayes' Rule: $P(x|y)= {P(y|x) \\over P(u)} P(x)$
- Causal Inference: $P(c|e)= {P(e|c) P(c) \\over P(e)}$ where $c$ = cause and $e$ = effect
- $E(X) = \\sum\\limits\_{x\\in X}P(x)\*x$
- $E(X) = {1\\over n} \\sum\\limits\_{i=1}^n x_i$
- $Var(X) = {1\\over n} \\sum\\limits\_{i=1}^n (x_i - mean(x))^2$
- Counting *with* replacement: $n^k$ (choice of $n$ things $k$ times)
- Counting *without* replacement:
  - Order Matters: $\_n P_k = {n! \\over (n-k)!}$
  - Order Does not Matter: $\_n C_k = {n! \\over (n-k)!k!}$
- Independent Conditional Distribution: $P(X|Y) = P(X)$ and $P(Y|X) = P(Y)$
- Independent Joint Distribution: $P(X,Y) = P(X)P(Y)$
- Bayesian Reasoning for Hypothesis: $P(H|E) = {P(E|H)P(H)\\over P(E)}$, where $E$ is data and $H$ is hypothesis
- Conditional Independence: $P(X|Y,Z)=P(X|Z)$ or $P((X\\cap Y)|Z)=P(X|Z)\*P(Y|Z)$

#### Examples:

- Layla is taking this quiz and doesn't know the solution to four of the multiple choice questions (each with three choices). If Layla randomly selects an answer, what is the probability that she gets all four questions right? $1/81$
- How many unique ways are there to arrange 3 characters from the word "CSE40"? $\_6 P_3$
- Given $P(X|Z) = 0.4, P(Y|Z) = 0.2,$ and $P(X,Y|Z) = 0.08$ we cannot say anything about the independence of $X$ and $Y$