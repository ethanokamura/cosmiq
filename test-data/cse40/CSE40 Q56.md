# Q56 Cheat Sheet

**Challenges with data**

- Attribute value ambiguity (25%, 0.25, 25/100, etc. or brown, Brown, brownish), Name ambiguity, Data entry errors, Missing values, Changing attributes, Data formatting (dates), Abbreviations / data truncation

**Data Quality**

- Accurate, Complete, Consistent, Relevant, Valid, Timely, and Uniform. We get high quality data by collecting it well to start and cleaning it after the fact

**Data Cleaning** - The process of identifying, deleting, and/or replacing inconsistent or incorrect information from the data. This technique ensures high quality of processed data and minimizes the risk of incorrect or inaccurate conclusions.

**Data cleaning process:Eliminate duplicate / irrelevant / incorrect data**

- Duplicates from data entry errors / merging multiple data sources / attribute ambiguity
- Irrelevance from old data or data that only applies to another group **Fix structural errors**
- When data is multi-relational, there are often structural inegrity constraints that must be checked. (students, majors, member-of: we want to ensure each student has at least one major) **Handle missing data (Null, empty, or whitespace)**
- Assume functional form (ie linear). and use appropriate methods (ie linear regression)
- Drop data that has missing values or use interpolation (infer values using a classifier that predicts the values based on the values of other features)
- **Missing Data Mechanisms**
  - Missing Completely at Random (MCAR) - there is no relationship between the missingness of the data and any values, observed or missing
  - Missing at Random (MAR) - there is a *systematic* relationship between the propensity of missing values and *observed* data, but *not* the missing data. (ie) if women are more likely to tell you their age than men, age is MAR
  - Missing Not at Random (MNAR) - there is a relationship between the propensity of a value to be missing and its value. (ie) people with lower education levels may be more likely to omit the education level field **Identify and review outliers**
- **Outliers** - a value or point that differs substantially from the rest of the data
- **Domain Knowledge** (Sometimes the typical ranges of a value are known):
  - For example, when measuring blood pressure, a doctor likely has a good idea of what is considered to be within the normal blood pressure range.
  - observations that fall outside the range may be worth investigation
- **Statistical Indicators** (we define outliers in reference to the data we're using)
  - Distance from the mean in standard deviations
  - Distance from the interquartile range by a multiple of the interquartile range **Standardize and normalize data** - technique applied to change the values of numeric columns in the dataset to use a common scale
- Normalization: Rescale the range of values for a given feature into a set range, such as \[0, 1\] or \[-1, 1\] (x' =   (x - min)  / (max - min))
- Standardization: rescale the range of values AND convert variance to a standard normal dist. with mean 0 and std 1.0
  - $x' = {(x-\\mu)\\over{\\sigma}}$ **Re-represent data (feature engineering)**
- **Feature transformation** - a set of methods for transforming existing data into a new representation that will lead to more effective learning
  - text -&gt; numeric (one-hot encoding)
  - numeric -&gt; categorical (binning aka discretization)
    - Binning can reduce the effective dimensionality of learning by grouping together similar values. Some algorithms don't handle continuous / numeric data well (or at all)
    - Poor choices of bin boundaries can mask important differences
  - many categories -&gt; fewer categories (grouping)
    - Methods: domain knowledge (genre, university type), clustering (co-clustering with class or other features), or wrapper methods (group, then learn, then regroup using feedback signal from learning)
- **Feature construction** - enriches data by adding derived features
  - mapping a feature into a new feature (using log functions) or creating a new feature from multiple features (using multiplication or addition)
- **Feature selection** - remove features from our data (identifying which variables are most relevant to the hypothesis)
  - Techniques: Manual Inspection or Automatic Techniques
    - Filter-based (use specific metric to filter features)
    - Wrapper-based: Treat the selection of set of features as a search problem
    - Embedded: Use a machine learning algorithm that has a built-in feature selection method
  - Leads to better, more efficient models
  - **The Pearson correlation coefficient** - the most common way of measuring a linear correlation (number between -1 and 1 that measures the strength and direction of the relationship)
  - **Chi-Squared Test** - used to determine if there is an association between *two categorical variables* (does not work for continuous var)
    - Determines if there is a statistically significant relationship **Math**
- These tools are essential for the basics of machine learning:
  - Linear regression - Summarize the relationship or “trend” between variables
  - Optimization - Find the best hypothesis in a space of possibilities
  - Gradient descent - Iteratively move towards better hypotheses / parameters

**Linear Algebra:** Model relationships between variables

- **vector norm**: a distance measure in a vector space that satisfies certain properties
  - useful for measuring the "size" of a vector and computing the distance between vectors
- L1-norm (Manhattan distance) is the sum of the absolute values of the individual feature values
- L2-norm (Euclidean distance) the square root of the sum of squares
- Dot product (scalar product) is a measure of how similar or aligned the vectors are

**Linear Regression** - mathematical way to draw a "trend" line through data in a scatter plot (line of best fit)

- we can use this line to make predictions in the data
- optimization of linear regression: find h such that $argmin\_{h \\in H} \\sum\\limits\_{x_i} l(h(x\_{i}),y\_{i})$ where $h$ is a line
- L1 loss: the absolute value of the diff between the prediction $\\hat{y_i}$ and the true label $y_i$
  - $|y_i - \\hat y_i|$
- L2 : (mean-squared error (MSE)): $(y_i - \\hat y_i)^2$
- Finding the best hypothesis (using linear algebra): $A^T Ax = A^T b$ (least squares solution of $Ax=b$)

**Calculus:** Model change in values over time or in relationship to other variables

- gradient: derivative of a multidimensional function
- capture the *local slope* of the function, allowing predictions of taking a small step from a point in any direction Convex function will have a single minimum (guaranteed)
- With gradients, if the derivative (in all directions) is zero, we have found the minimum

**Gradient Descent** (Most popular optimization alg)

- Important design choices: Initial value and learning rate
- Depending on where we start, we will follow a different path to reach the valley ($x_0$)
- Depending on the speed we might arrive at the valley in a different manner ($\\alpha$)
- Gradient descent gives us:
  1. Direction of the slope
  2. Step size (how many unites to move in a particular direction)
     - $x\_{n+1} = x_n - \\alpha {d\\over dx_n}f(x_n)$ where $x\_{n+1}$ is the new position $x_n$ is old position, $\\alpha$ is learning rate and ${d\\over dx_n}f(x_n)$ is the gradient of function f for $\\theta_n$
- Good step size converges quickly, large step size diverges, tiny step size converges slowly, large step size converges slowly, (fails for non-continuous functions)